{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chennels_df = pd.read_csv('data/chennels_df.csv')\n",
    "chennels_df['chennel@'] = chennels_df['hrefs'].apply(lambda x: x[26:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<telethon.client.telegramclient.TelegramClient at 0x1bbdc8199e8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from telethon import TelegramClient, events, sync\n",
    "from telethon.tl.functions.messages import GetHistoryRequest\n",
    "from telethon.tl.functions.channels import JoinChannelRequest,LeaveChannelRequest\n",
    "\n",
    "# These example values won't work. You must get your own api_id and\n",
    "# api_hash from https://my.telegram.org, under API Development.\n",
    "api_id = 1522790\n",
    "api_hash = '00008593fd02991e904e01df2abe2310'\n",
    "\n",
    "client = TelegramClient('session_name', api_id, api_hash)\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_n_messages(channel_name, n, client):\n",
    "    channel = client.get_entity(channel_name)\n",
    "    posts = client(GetHistoryRequest(\n",
    "    peer=channel,\n",
    "    limit=n,\n",
    "    offset_date=None,\n",
    "    offset_id=0,\n",
    "    max_id=0,\n",
    "    min_id=0,\n",
    "    add_offset=0,\n",
    "    hash=0))\n",
    "\n",
    "    post_texts = [post.message for post in posts.messages]\n",
    "    chennel_posts_df = pd.DataFrame(zip(post_texts,posts.messages), columns = ['text', 'message'])\n",
    "    return chennel_posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['news', 'blogs', 'tech', 'entertainment', 'economics', 'crypto',\n",
       "       'education', 'music', 'language', 'business', 'psychology',\n",
       "       'marketing', 'career', 'video', 'books', 'fitness', 'travel',\n",
       "       'art', 'beauty', 'health', 'gaming', 'food', 'sales', 'quotes',\n",
       "       'handicraft', 'adult', 'other', 'uzbek'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chennels_df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['news', 'blogs', 'economics', 'crypto',\n",
    "       'education', 'tech', 'psychology',\n",
    "       'marketing', 'career', 'other']\n",
    "\n",
    "chennels_df_to_parse = chennels_df[chennels_df['class'].isin(classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Канал @nexta_live был успешно спаршен и классифицирован\n",
      "Канал @nexta_tv был успешно спаршен и классифицирован\n",
      "Канал @breakingmash был успешно спаршен и классифицирован\n",
      "Канал @topor был успешно спаршен и классифицирован\n",
      "Канал @petrovtel был успешно спаршен и классифицирован\n",
      "Канал @DavydovIn был успешно спаршен и классифицирован\n",
      "Канал @tutby_official был успешно спаршен и классифицирован\n",
      "Канал @rhymestg был успешно спаршен и классифицирован\n",
      "Канал @meduzalive был успешно спаршен и классифицирован\n",
      "Канал @lentachold был успешно спаршен и классифицирован\n",
      "Канал @stopcoronavirusrussia был успешно спаршен и классифицирован\n",
      "Канал @bazabazon был успешно спаршен и классифицирован\n",
      "Канал @corona был успешно спаршен и классифицирован\n",
      "Канал @otsuka_bld был успешно спаршен и классифицирован\n",
      "Канал @mkbelarus был успешно спаршен и классифицирован\n",
      "Канал @oldlentach был успешно спаршен и классифицирован\n",
      "Канал @luxta_tv был успешно спаршен и классифицирован\n",
      "Канал @COVID2019_official был успешно спаршен и классифицирован\n",
      "Канал @moscowmap был успешно спаршен и классифицирован\n",
      "Канал @belteanews был успешно спаршен и классифицирован\n",
      "Запустилось ожидание 120.0 минут\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d45bd8a5b88c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Запустилось ожидание {} минут'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDAY_TIME\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatches_count\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDAY_TIME\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatches_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "DAY_TIME = 60*60*24\n",
    "\n",
    "while True: # 1 day cycle\n",
    "    count_chenells = len(chennels_df_to_parse)\n",
    "    batches_count = count_chenells//20\n",
    "    if count_chenells%20 != 0:\n",
    "        batches_count += 1\n",
    "        \n",
    "    for k in range(batches_count):\n",
    "        batch_to_parse = chennels_df_to_parse.iloc[k*20:(k+1)*20]\n",
    "        \n",
    "        for i, (href, name, class_name, channel_name) in batch_to_parse.iterrows():\n",
    "\n",
    "            client(JoinChannelRequest(channel_name))\n",
    "            chennel_posts_df = parse_n_messages(channel_name, 10, client)\n",
    "            chennel_posts_df['class'] = class_name\n",
    "            chennel_posts_df['channel'] = channel_name\n",
    "            chennel_posts_df['id'] = chennel_posts_df['message'].apply(lambda x: x.id)\n",
    "            chennel_posts_df['target_cat'] = predict(chennel_posts_df)\n",
    "\n",
    "            posts_df = pd.concat([posts_df,chennel_posts_df], axis = 0)\n",
    "            posts_df = posts_df.drop_duplicates(['id','channel'])    \n",
    "            \n",
    "            client(LeaveChannelRequest(channel_name))\n",
    "            \n",
    "            print('Канал {} был успешно спаршен и классифицирован'.format(channel_name))\n",
    "            posts_df.to_csv('data/new_posts_df.csv', index = False)\n",
    "        print('Запустилось ожидание {} минут'.format(DAY_TIME/batches_count//60))\n",
    "\n",
    "        time.sleep(DAY_TIME/batches_count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели и функция классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import stop_words\n",
    "import nltk\n",
    "from nltk.stem import  SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_ru = set(stopwords.words('russian'))\n",
    "stop =  stop_ru | set(stop_words.get_stop_words('ru')) -set('год')\n",
    "\n",
    "stemmer_ru = SnowballStemmer('russian')\n",
    "stemmer_eng = SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/posts_df.csv')\n",
    "df['business'] = (df['class']=='business').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.text.notnull())]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "def string_transform(string):\n",
    "    string = string.lower().split()\n",
    "    string = [stemmer_eng.stem(stemmer_ru.stem(i)) for i in string if i not in stop]\n",
    "\n",
    "    return ' '.join(string)\n",
    "\n",
    "df['text'] = df['text'].apply(string_transform)\n",
    "\n",
    "text_tf_idf = TfidfVectorizer(min_df=0.01, ngram_range = (1,2))\n",
    "text_data = text_tf_idf.fit_transform(df['text'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cat = 'business'\n",
    "model.fit(text_data, df[target_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    df['new_text'] = df['text'].apply(string_transform)\n",
    "    text_data = text_tf_idf.transform(df['new_text'].astype(str))\n",
    "    preds = model.predict(text_data)\n",
    "    return preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
